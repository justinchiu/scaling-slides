
%\RequirePackage{pdf15}

\documentclass{beamer}

\usepackage[utf8]{inputenc}

\usepackage{mystyle}

\usepackage{natbib}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{subcaption}

\usepackage{tikz-dependency}
\usetikzlibrary{shapes.arrows, positioning, fit, bayesnet,
    arrows,backgrounds,patterns,matrix,calc,shadows,plotmarks,
    shapes,positioning,automata,positioning,spy,scopes,chains,decorations,decorations.pathreplacing}

\newcommand{\FancyUpArrow}{\begin{tikzpicture}[baseline=-0.3em]
\node[single arrow,draw,rotate=90,single arrow head extend=0.2em,inner
ysep=0.2em,transform shape,line width=0.05em,top color=green,bottom color=green!50!black] (X){};
\end{tikzpicture}}
\newcommand{\FancyDownArrow}{\begin{tikzpicture}[baseline=-0.3em]
\node[single arrow,draw,rotate=-90,single arrow head extend=0.2em,inner
ysep=0.2em,transform shape,line width=0.05em,top color=red,bottom color=red!50!black] (X){};
\end{tikzpicture}}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

%Information to be included in the title page:
\title{Scaling Hidden Markov Models}
%\author{Justin T Chiu and Alexander Rush\\Cornell Tech}

\setbeamertemplate{navigation symbols}{} 

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Latent Variables Models in NLP}
Chicken and egg problem:
\vspace{2em}
\begin{itemize}
\item NLP benchmarks are dominated by fully observed models
\vspace{2em}
\item Most tasks are fully supervised
\vspace{2em}
\item Evaluation of unsupervised tasks is also difficult
\vspace{2em}
\item Goal: Demonstrate efficacy of latent variable models by
    making them competitive on an existing task
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Language Modeling}
\begin{itemize}
\item Given the words seen so far, predict the next word
\vspace{2em}
\item Language requires modeling long-range phenomena
\end{itemize}
PICTURE
\end{frame}

\begin{frame}
\frametitle{Research Question}
\begin{itemize}
\item How far can we scale simple latent variables models?
\vspace{2em}
\item Under the assumption that tasks will only be developed if models are
    reasonably performant
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Hidden Markov Models in NLP}
\begin{itemize}
\item Simplest latent variable models for time series data
\vspace{2em}
\item Are thought to be very poor language models
\vspace{2em}
\item We show they are better than previously thought once scaled
\end{itemize}
\end{frame}

\section{Background: HMMs}

\begin{frame}
\frametitle{HMMs}

For times $t$, model states $z_t \in [Z]$, and tokens $x_t \in [X]$,

\begin{center}
\begin{tikzpicture}[]
\node[latent] (z0) {$z_0$} ;
\node[latent] (z1) [right=1.25cm of z0] {$z_1$} ;
\node[latent] (z2) [right=1.25cm of z1] {$z_2$} ;

\node[obs]    (x0) [below = 0.75cm of z0] {$x_0$};
\node[obs]    (x1) [below = 0.75cm of z1] {$x_1$};
\node[obs]    (x2) [below = 0.75cm of z2] {$x_2$};

\edge {z0} {x0};
\edge {z1} {x1};
\edge {z2} {x2};
\edge {z0} {z1};
\edge {z1} {z2};
\end{tikzpicture}
\end{center}

\begin{itemize}
\item Joint distribution $p(x,z) = \prod_t p(x_t \mid z_t)p(z_t \mid z_{t-1})$
\vspace{1em}
\item Start vector $\pi\in[0,1]^{Z}$, with $[\pi]_{z_0} = p(z_0)$
\vspace{1em}
\item Transition matrix $A\in[0,1]^{Z \times Z},$ with $[A]_{z_{t-1},z_t} = p(z_t \mid z_{t-1})$
\vspace{1em}
\item Emission matrix $O\in[0,1]^{Z \times X}$, with $[O]_{z_t, x_t} = p(x_t \mid z_t)$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Inference}
Given observed $x = (x_1, \ldots, x_T)$
\vspace{1em}
We wish to maximize
\begin{equation*}
p(x) = \sum_{z_1}\cdots\sum_{z_T}p(x, z),
\end{equation*}
computed via the forward algorithm
\vspace{2em}
\begin{itemize}
\item Start $\alpha_1 = \pi [O]_{\cdot,x_1}$
\item Transition operators $\Lambda_t = A\diag([O]_{\cdot,x_t})\in[0,1]^{Z \times Z}$
\item Forward algorithm computes evidence
$$p(x) = \underbrace{\alpha_1^\top\Lambda_2\Lambda_3}_{\alpha_3}\cdots\Lambda_T\bm1$$
\item Each $[\alpha_t]_{z_t} = p(z_t, x_{\le t})$
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{Inference}
%\begin{itemize}
%\end{itemize}
%\end{frame}

\begin{frame}
\frametitle{Inference}
\begin{columns}
\begin{column}{0.5\textwidth}  %%<--- here
\begin{itemize}
    \item Nodes correspond to states
    \vspace{2em}
    \item Edges to entries in $\Lambda_t$
    \vspace{2em}
    \item Sequentially compute posterior state probabilities
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}
\begin{center}
\resizebox{0.8\width}{0.8\height}{
\input{img/diagram_full.tex}
}
\end{center}
\end{figure}
\end{column}
\end{columns}
\end{frame}

\section{Scaling HMMs}

\begin{frame}
\frametitle{Lessons from Large Neural Language Models}

Large models perform better but are \ldots
\vspace{2em}
\begin{enumerate}
\item Slow to train
\vspace{2em}
\item Prone to overfitting
\end{enumerate}
\vspace{2em}
We must overcome these issues when scaling HMMs
\end{frame}


\begin{frame}
\frametitle{4 Techniques for Training Large HMMs}
\begin{itemize}
\item Compact neural parameterization
\vspace{1em}
    \begin{itemize}
    \item[] \FancyUpArrow~Generalization
    \end{itemize}
\vspace{1em}
\item State dropout
\vspace{1em}
    \begin{itemize}
    \item[] \FancyUpArrow~Speed \quad \FancyUpArrow~Generalization
    \end{itemize}
\vspace{1em}
\item Block-sparse emission constraints
\vspace{1em}
    \begin{itemize}
    \item[] \FancyUpArrow~Speed
    \end{itemize}
\vspace{1em}
\item Kernel-based generalized softmax
\vspace{1em}
    \begin{itemize}
    \item[] \FancyUpArrow~Speed
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Technique 1: Neural Parameterization}
\begin{itemize}
\item A neural parameterization allows for parameter sharing
\item Generate conditional distributions from
state $\mathbf{E}_z$ and token representations $\mathbf{E}_x$
\end{itemize}
\begin{center}
\includegraphics[height=1.7in]{img/blocksparse_mat_no_block.pdf}
\end{center}

REDO. picture should show $p(x \mid z) = \exp(L R^\top)$,
generating L and R from neural networks

\end{frame}

\begin{frame}
\frametitle{Technique 2: State Dropout}
\begin{itemize}
\item State dropout encourages broad state usage
\item At each batch, sample dropout mask $\mathbf{b} \in \set{0,1}^{Z}$
\end{itemize}
\begin{center}
\resizebox{0.75\width}{0.75\height}{
\input{img/trellis.tex}
}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Technique 3: Block-Sparse Emission Constraints}
This slide sucks, redo
\begin{itemize}
\item Reduce cost of marginalization by enforcing structure
\vspace{0.5em}
\item Partition words and states jointly
\vspace{0.5em}
\item Words can only be emit by states in the aligned group
\end{itemize}
\begin{center}
\begin{tikzpicture}[every node/.style={anchor=base,minimum size=8mm}]
    \matrix  (graph) [matrix of nodes, row sep=0.5em,column sep=-0.3em,
    minimum width=2em, minimum height=2em, font=\small,ampersand replacement=\&,
    nodes in empty cells
    ] {
        \&$\mcX_1$ \&
        \&
        \textcolor{green!10}{a}\&
        $\mcX_2$ \&
        \textcolor{green!10}{a}\&
        \&
        $\cdots$ \&
        \&
        $\mcX_M$ \\
        \textcolor{blue!80}{$p(x\mid z)$} \& \& \& \&\& \&\&\&\&\\
        \&$\mcZ_1$ \&
        \&
        $\mcZ_2$ \&
        \&
        $\cdots$ \&
        \&
        $\mcZ_M$ \&\&\\
    };
    
    \begin{scope}[on background layer]
      \draw[blue!30, line width=0.5mm] (graph-1-2) -- (graph-3-2);
      \draw[blue!30,line width=0.5mm] (graph-1-5) -- (graph-3-4);
      \draw[blue!30,line width=0.5mm] (graph-1-10) -- (graph-3-8);

      %\draw[draw=red!30, line width=0.2mm] ($(graph-1-1) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      %\draw[draw=red!30, line width=0.2mm] ($(graph-1-2) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      %\draw[draw=red!30, line width=0.1mm] ($(graph-1-3) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      %\draw[draw=red!30, line width=0.9mm] ($(graph-1-4) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      %\draw[draw=red!30, line width=0.1mm] ($(graph-1-5) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      %\draw[draw=red!30, line width=0.4mm] ($(graph-1-6) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      %\draw[draw=red!30, line width=0.1mm] ($(graph-1-7) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);

      \draw[rounded corners,fill=red!10] ($ (graph-3-2.north west) +(-0.1,0.1)$) rectangle  node[yshift=-0.8cm]{\textcolor{red}{}} ($(graph-3-8.south east) +(0.1,-0.1)$ ) ;
      \draw[rounded corners,fill=red!10] ($ (graph-1-2.north west) +(-0.1,0.1)$) rectangle  node[yshift=0.8cm]{\textcolor{red}{}} ($(graph-1-10.south east) +(0.1,-0.1)$ ) ;

      \draw[rounded corners,fill=green!10] (graph-1-2.north west) rectangle  node[ yshift =0.7cm] {} (graph-1-2.south east);
      \draw[rounded corners,fill=green!10] (graph-1-4.north west) rectangle  node[ yshift =0.7cm] {} (graph-1-6.south east);
      \draw[rounded corners,fill=green!10] (graph-1-10.north west) rectangle  node[ yshift =0.7cm] {} (graph-1-10.south east);

      \draw[rounded corners, fill=blue!10] (graph-3-2.north west) rectangle  node[yshift=-0.9cm]{}  ($(graph-3-2.south east)$);
      \draw[rounded corners, fill=blue!10] (graph-3-4.north west) rectangle  node[yshift=-0.9cm]{}  ($(graph-3-4.south east)$);
      \draw[rounded corners,fill=blue!10] (graph-3-8.north west) rectangle  node[ yshift =0.7cm] {} (graph-3-8.south east);
      %\path[] (graph-3-3.north west) rectangle  node[yshift=-0.9cm]{\textbf{$y_3$}} (graph-3-3.south east);
    \end{scope}
  \end{tikzpicture}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Block-Sparse Emissions: Effect on Inference}

Given each word $x_t$, only the states in the correct group can occur
\vspace{1em} 

\begin{figure}
\captionsetup[subfigure]{justification=centering}
\begin{center}
\begin{subfigure}[t]{0.45\textwidth}
\centering
\resizebox{0.75\width}{0.75\height}{
\input{img/diagram_full.tex}
}
\caption{No constraints}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\centering
\resizebox{0.75\width}{0.75\height}{
\input{img/trellis_nodrop.tex}
}
\caption{Block-sparse emission}
\end{subfigure}
\end{center}
\end{figure}

\end{frame}

\begin{frame}
\frametitle{Technique 4: Generalized Softmax}
Focusing on the transition distribution,
\vspace{1em}
\begin{itemize}
\item Softmax
$$p(z_t \mid z_{t-1}) = \frac{\exp(\bu_{z_{t-1}}^\top \bv_{z_{t}})}
{\sum_z \exp(\bu_{z_{t-1}}^\top \bv_z)}$$
\vspace{1em}
\item Generalized Softmax
$$p(z_t \mid z_{t-1})
= \frac{K(\bu, \bv)}{\sum_z K(\bu,\bv_z)}
= \frac{\phi(\bu)^\top\phi(\bv)}
    {\sum_z \phi(\bu)^\top\phi(\bv_z)},$$
for positive kernel $K:\R^d\times\R^d\to\R_+$
and feature map $\phi:\R^d\to\R^f$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Generalized Softmax: Inference}
\begin{itemize}
\item The key $O(Z^2)$ step in the forward algorithm:
$$
p(z_t \mid x_{<t}) = \sum_{z_{t-1}} p(z_t \mid z_{t-1})p(z_{t-1} \mid x_{<t})
$$
\vspace{1em}
\item In matrix form, 
\begin{equation*}
\bm\gamma_t = \underbrace{\bm\alpha_{t-1}}_{\R^Z}\underbrace{\Lambda_{}}_{\R^{Z \times Z}},
\end{equation*}
where we have the probability of the
\begin{align*}
&\textnormal{current state, } & [\bm\gamma_{t}]_{z_{t}} &= p(z_{t} \mid x_{< t}),\\
&\textnormal{last state, } & [\bm\alpha_{t-1}]_{z_{t-1}} &= p(z_{t-1} \mid x_{< t}),\\
&\textnormal{transition probability, } & [\Lambda]_{z_{t-1},z_t} &= p(z_t\mid z_{t-1})
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Generalized Softmax: Inference}
\begin{itemize}
\item Use generalized softmax in transition distribution
$$[\Lambda]_{z_{t-1},z_t} = p(z_t\mid z_{t-1}) \propto \phi(\bu_{z_{t-1}})^\top\phi(\bv_{z_t})$$
\item Allows us to apply associative property of matrix multiplication
\begin{align*}
\bm\gamma_t
&= \bm\alpha_{t-1}\Lambda\\
&= \bm\alpha_{t-1}(\textrm{diag}(d)\phi(U)\phi(V)^\top)\\
&= \underbrace{(\bm\alpha_{t-1}\circ d)}_{\R^Z}
\underbrace{\phi(U)}_{\R^{Z \times f}}
\underbrace{\phi(V)^\top}_{\R^{f \times Z}},
\end{align*}
with stacked embeddings $\phi(U),\phi(V) = [\phi(\bv_1), \ldots,\phi(\bv_Z)]$
and normalizing constants $d$
\vspace{1em}
\item Takes $O(Zf)$ time from left to right!
\end{itemize}
\end{frame}


\section{Experiments}
\begin{frame}
\frametitle{Experiments}
\begin{itemize}
\item Language modeling on Penn Treebank
\vspace{1em}
\item Baselines
    \begin{itemize}
    \item Feedforward 5-gram model
    \item 2-layer LSTM
    \item A 900 state HMM (Buys et al 2018)
    \end{itemize}
\vspace{1em}
\item Model
    \begin{itemize}
    \item $2^{15}$ (32k) state very large HMM (VL-HMM)
    \item $M=128$ groups (256 states per type), obtained via Brown Clustering
    \item Dropout rate of $0.5$ during training
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Results on PTB Validation Data}
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar=5pt,
    %x=4cm,
    %enlarge x limits=0.6,
    ylabel={Validation Perplexity},
    symbolic x coords={FF, LSTM, VL-HMM, HMM},
    xticklabels={FF, LSTM, VL-HMM, HMM},
    xtick={FF, LSTM, VL-HMM, HMM},
    tick pos=left,
    every axis plot/.append style={
      ybar,
      bar shift=0pt,
      fill
    },
    nodes near coords,
    nodes near coords align={vertical},
    bar width=2em,
    ]

\addplot coordinates {(FF,159.9)};
\addplot coordinates {(LSTM,93.6)};
\addplot coordinates {(VL-HMM,115.8)};
\addplot coordinates {(HMM,284.6)};

\end{axis}
\end{tikzpicture}
\end{frame}

\begin{frame}
\frametitle{State Size Ablation}

\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel=State Size $Z$,
    ylabel=Validation PPL,
    xmode=log,
    log basis x={2},
    xtick={},
    tick pos=left,
    width=7cm,
    height=5cm
]
\addplot plot coordinates {
(1024,  213.25)
(2048,  199.98)
(4096,  169.18)
(8192,  150.22)
(16384, 135.79)
(32768, 125.02)
(65536, 121.93)
};
\end{axis}
\end{tikzpicture}

\vspace{2em}

Validation perplexity on PTB by state size ($\lambda =0.5$ and $M=128$)
\end{frame}

\begin{frame}
\frametitle{Other Ablations}

\begin{center}
\begin{tabular}{lrrr}
\toprule
Model                 & Param & Train & Val\\
\midrule
VL-HMM ($2^{14}$)     & 7.2M & 115    & 134 \\
\quad - neural param  & 423M & 119    & 169 \\
\quad - state dropout & 7.2M & 88     & 157 \\
\bottomrule
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Experiments}
\begin{itemize}
\item Language modeling on PTB
\vspace{2em}
\item Work directly with feature map $\phi(\bx) = \exp\left(W\bx - \frac{\|x\|^2}{2}\right)$,
with learned $W\in\R^{d \times f}$
\vspace{2em}
\item No dropout or sparsity constraints
\end{itemize}
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{Results on PTB Validation}
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel=Number of features $f$,
    ylabel=Validation PPL,
    xmode=log,
    log basis x={2},
    xtick={},
    tick pos=left,
    width=7cm,
    height=5cm,
    legend pos=outer north east
]
\addplot plot coordinates {
(1024,  163)
(2048,  163)
(4096,  157)
(8192,  156)
};
\addplot plot coordinates {
(1024,  167)
(2048,  162)
(4096,  162)
(8192,  155)
};
\legend{$Z=16k$,$Z=8k$}
\end{axis}
\end{tikzpicture}
\end{frame}
\end{comment}

\begin{comment}
\begin{frame}
\frametitle{Results on PTB Validation}
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel=Number of features $f$,
    ylabel=Validation PPL,
    xmode=log,
    log basis x={2},
    xtick={},
    tick pos=left,
    width=7cm,
    height=5cm,
    legend pos=outer north east
]
\addplot plot coordinates {
(128,  186)
(256,  180)
(512,  177)
(1024,  163)
};
\addplot plot coordinates {
(128,  195)
(256,  182)
(512,  177)
(1024,  166)
};
\addplot plot coordinates {
(128,  192)
(256,  180)
(512,  179)
(1024,  177)
};
\legend{$Z=4k$,$Z=2k$,$Z=1k$}
\end{axis}
\end{tikzpicture}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{Results on PTB Validation}
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel=Number of states $Z$,
    ylabel=Validation PPL,
    xmode=log,
    log basis x={2},
    xtick=data,
    xticklabels={1024,2048,4096,8192},
    tick pos=left,
    width=7cm,
    height=5cm,
    legend pos=outer north east
]
\addplot plot coordinates {
(1024,  182)
(2048,  172)
(4096,  165)
(8192,  160)
};
\addplot plot coordinates {
(1024,  177)
(2048,  166)
(4096,  163)
(8192,  164)
};
\addplot plot coordinates {
(1024,  179)
(2048,  177)
(4096,  177)
(8192,  168)
};
\addplot plot coordinates {
(1024,  180)
(2048,  182)
(4096,  180)
(8192,  177)
};
%\addplot plot coordinates {
%(1024,  192)
%(2048,  195)
%(4096,  186)
%(8192,  191)
%};
\legend{softmax,$f=1024$,$f=512$,$f=256$}
%\legend{softmax,$f=1024$,$f=512$,$f=256$,$f=128$}
\end{axis}
\end{tikzpicture}
\vspace{1em}
\begin{itemize}
    \item Holding number of features fixed, perplexity mostly improves
        or remains the same with an increasing number of states
    \item Achieve similar performance as softmax with around 4:1 state to feature ratio
        (also holds for 8k and 16k states)
    %\item Results are a bit noisy, hoping dropout will clean things up
    %\item Currently running that plus larger state sizes
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conclusion}
\begin{itemize}
\item Hopeful that HMMs can be competitive language models
\vspace{2em}
\item Introduced 4 techniques for tackling speed and overfitting
\vspace{2em}
\item Future work will extend to other discrete latent variable models
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{EOS}
\end{frame}


\begin{frame}
\frametitle{Citations}
\bibliographystyle{acl_natbib}
\bibliography{anthology,emnlp2020}
\end{frame}

\end{document}
