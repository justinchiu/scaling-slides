
%\RequirePackage{pdf15}

\documentclass{beamer}

\usepackage[utf8]{inputenc}

\usepackage{mystyle}

\usepackage{natbib}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{subcaption}

\usepackage{tikz-dependency}
\usetikzlibrary{shapes.arrows, positioning, fit, bayesnet,
    arrows,backgrounds,patterns,matrix,calc,shadows,plotmarks,
    shapes,positioning,automata,positioning,spy,scopes,chains,decorations,decorations.pathreplacing}

\newcommand{\FancyUpArrow}{\begin{tikzpicture}[baseline=-0.3em]
\node[single arrow,draw,rotate=90,single arrow head extend=0.2em,inner
ysep=0.2em,transform shape,line width=0.05em,top color=green,bottom color=green!50!black] (X){};
\end{tikzpicture}}
\newcommand{\FancyDownArrow}{\begin{tikzpicture}[baseline=-0.3em]
\node[single arrow,draw,rotate=-90,single arrow head extend=0.2em,inner
ysep=0.2em,transform shape,line width=0.05em,top color=red,bottom color=red!50!black] (X){};
\end{tikzpicture}}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

%Information to be included in the title page:
\title{Scaling Hidden Markov Models}
%\author{Justin T Chiu and Alexander Rush\\Cornell Tech}

\setbeamertemplate{navigation symbols}{} 

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Latent Variables Models in NLP}
\begin{itemize}
\item NLP benchmarks are dominated by fully observed models
\vspace{2em}
    \begin{itemize}
    \item Transformers
    \vspace{2em}
    \item Previously, Recurrent Neural Networks
    \end{itemize}
\vspace{2em}
\item We instead explore latent variable models
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Latent Variable Models: Motivation}
\begin{itemize}
\item LVMs posit a generative process involving unseen variables
\vspace{2em}
\item Maintain uncertainty over latent representations,
    rather than just output correlations
\vspace{2em}
\item Often improves interpretability and controllability
\vspace{2em}
\item Bottlenecked by the computational complexity of inference
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Research Question}
To what extent is the performance of tractable
latent variable models limited by scale and choices in parameterization?

\vspace{2em}

\textbf{This work:} Scale hidden Markov models (HMMs) on
language modeling using techniques
drawn from recent advances in neural networks
\end{frame}

\begin{frame}
\frametitle{Language Modeling}
\begin{center}
\textrm{\large How now, brown \rule{1cm}{0.15mm}}
\end{center}
\vspace{2em}
\begin{itemize}
\item Given the words seen so far, predict the next word
\vspace{2em}
\item Language requires modeling long-range phenomena
\end{itemize}
\vspace{2em}
\end{frame}

\begin{frame}
\frametitle{Hidden Markov Models in NLP}
\begin{itemize}
\item Simplest latent variable models for time series data
\vspace{2em}
\item Are thought to be very poor language models
\vspace{2em}
\item We show they are better than previously thought, once scaled
\end{itemize}
\end{frame}

\section{Background: Hidden Markov Models}
\begin{frame}
\frametitle{Hidden Markov Models (HMMs)}
\begin{itemize}
\item Classical models for unsupervised per-word tag induction
\vspace{1em}
    \begin{itemize}
    \item Part-of-speech induction
    \vspace{1em}
    \item Word alignment for translation
    \end{itemize}
\vspace{1em}
\item Admits tractable exact inference
\vspace{1em}
    \begin{itemize}
    \item Strong conditional independence assumptions
    \vspace{1em}
    \item Finite set of discrete latent states
    \end{itemize}
    % remove confound of approximate inference
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Hidden Markov Models (HMMs)}

For times $t$, model states $z_t \in [Z]$, and tokens $x_t \in [X]$,

\begin{center}
\begin{tikzpicture}[]
\node[latent] (z0) {$z_1$} ;
\node[latent] (z1) [right=1.25cm of z0] {$z_2$} ;
\node[latent] (z2) [right=1.25cm of z1] {$z_3$} ;

\node[obs]    (x0) [below = 0.75cm of z0] {$x_1$};
\node[obs]    (x1) [below = 0.75cm of z1] {$x_2$};
\node[obs]    (x2) [below = 0.75cm of z2] {$x_3$};

\edge {z0} {x0};
\edge {z1} {x1};
\edge {z2} {x2};
\edge {z0} {z1};
\edge {z1} {z2};
\end{tikzpicture}
\end{center}

This yields the joint distribution
$$p(x,z) = \prod_t p(x_t \mid z_t)p(z_t \mid z_{t-1})$$
%with distributional parameters
%\begin{align*}
%&\textnormal{start state vector } & \pi\in[0,1]^{Z},
%    & \textnormal{ with } &[\pi]_{z_0} &= p(z_0),\\
%&\textnormal{transition matrix } & A\in[0,1]^{Z \times Z},
%    & \textnormal{ with } &[A]_{z_{t-1},z_t} &= p(z_t \mid z_{t-1}),\\
%&\textnormal{emission matrix } & O\in[0,1]^{Z \times X},
%    & \textnormal{ with } &[O]_{z_t, x_t} &= p(x_t \mid z_t)\\
%\end{align*}
%
with 
\begin{align*}
&\textnormal{start state } & p(z_1)&,\\
&\textnormal{transitions } & p(z_t \mid z_{t-1})&,\\
&\textnormal{and emissions } &  p(x_t \mid z_t)&
\end{align*}
represented as vectors and matrices

\end{frame}



\begin{frame}
\frametitle{Inference}
Given observed $x = (x_1, \ldots, x_T)$
\vspace{1em}
We wish to maximize
\begin{equation*}
p(x)
= \sum_{z_1}\cdots\sum_{z_T}p(x, z)
= \alpha_1^\top\Lambda_2\Lambda_3\cdots\Lambda_T\bm1,
\end{equation*}
where we have the
\begin{align*}
&\textnormal{start, } & [\alpha_1]_{z_1} &= p(x_1\mid z_1)p(z_1),\\
&\textnormal{and transition operators, }
    & [\Lambda_t]_{z_{t-1},z_t} &= p(x_t \mid z_t)p(z_t \mid z_{t-1})
\end{align*}
The result of each matvec has the alphas of the forward algorithm,
i.e. $\alpha_3 = \alpha_1\Lambda_2\Lambda_3$ has entries corresponding to $p(z_3, x_{1:3})$
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{Transition Operators}
skip slide? mainly to make the connection between distributional parameters
and inference really concrete.
\begin{itemize}
    \item The transition operators $\alpha_1,\Lambda_t$ are functions of
        the distributional parameters: starting state $\pi$, state transitions $A$,
            and emissions $O$
    \vspace{1em}
    \item In particular
\begin{align*}
&\textnormal{the start } & \alpha_1 &= \pi \circ [O]_{\cdot,x_1},\\
&\textnormal{and the transition operators }
    & \Lambda_t &= A\diag([O]_{\cdot, x_t})
\end{align*}
\end{itemize}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{Inference}
%Given the start and transition operators, we can visualize inference
$$p(x) = \alpha_1^\top\Lambda_2\cdots\Lambda_T\mathbf{1}$$
\begin{figure}
\begin{center}
\resizebox{0.8\width}{0.8\height}{
\input{img/diagram_full.tex}
}
\end{center}
\end{figure}
\vspace{-1em}
\begin{itemize}
    \item Each node corresponds to a state
    %\vspace{1em}
    \item Each edge to an entry in the transition operator matrix
\end{itemize}
\end{frame}

\section{Scaling HMMs}

\begin{frame}
\frametitle{Lessons from Large Neural Language Models}

Large models perform better but are \ldots
\vspace{2em}
\begin{enumerate}
\item Slow to train
\vspace{2em}
\item Prone to overfitting
\end{enumerate}
\vspace{2em}
We must overcome these issues when scaling HMMs
\end{frame}

\begin{frame}
\frametitle{3 Techniques for Training Large HMMs}
\begin{itemize}
\item Compact neural parameterization
\vspace{1em}
    \begin{itemize}
    \item[] \FancyUpArrow~Generalization
    \end{itemize}
\vspace{1em}
\item State dropout
\vspace{1em}
    \begin{itemize}
    \item[] \FancyUpArrow~Speed \quad \FancyUpArrow~Generalization
    \end{itemize}
\vspace{1em}
\item Block-sparse emission constraints
\vspace{1em}
    \begin{itemize}
    \item[] \FancyUpArrow~Speed
    \end{itemize}
\vspace{1em}
\item Will cover a fourth in the second part of this talk
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Technique 1: Neural Parameterization}
\begin{itemize}
\item The transition and emission matrices have $Z^2$ and $ZX$ entries
\vspace{2em}
\item Causes the number of parameters to explode as the state size increases
\vspace{2em}
\item We instead use a low-dimensional decomposition of all conditional distributions
    that greatly reduces the number of parameters
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Neural Parameterization: Softmax}
For both the transition and emission matrices,
we use a softmax parameterization,
which assumes a nonlinear $D$-dimensional decomposition
\vspace{1em}
\[
\vcenter{\hbox{\begin{tikzpicture}[baseline=-0.5ex]
    \draw (0,0) rectangle (2,2) node[pos=.5] {$W$};
\end{tikzpicture}}}
\propto
\exp\left(\,
\vcenter{\hbox{\begin{tikzpicture}[baseline=-0.5ex]
    \draw (0,0) rectangle (1,2) node[pos=.5] {$U$};
\end{tikzpicture}}}
\times 
\vcenter{\hbox{\begin{tikzpicture}[baseline=-0.5ex]
    \draw (0,0) rectangle (2,1) node[pos=.5] {$V^\top$};
\end{tikzpicture}}}
\,\right)
\]
\vspace{1em}
with embeddings $U\in\R^{Z\times D}, V\in\R^{Z\times D}$ or $\R^{X \times D}$
\vspace{1em}
\begin{itemize}
\item Can further parameterize $U$ or $V = \textrm{MLP}(E_u)$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Technique 2: State Dropout}
\begin{itemize}
\item Dropout is a common technique for regularizing neural networks
\vspace{2em}
    \begin{itemize}
    \item Reduces a network's reliance on a particular neuron
    \end{itemize}
\vspace{2em}
\item Extend dropout to the states of an HMM
\vspace{2em}
    \begin{itemize}
    \item Encourage broad utilization of all states
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{State Dropout}
\begin{itemize}
\item At each batch, sample dropout mask $\mathbf{b} \in \set{0,1}^{Z}$
\item Compute distributional parameters by indexing into embeddings $U,V$
\end{itemize}

\begin{center}
\begin{figure}
\begin{subfigure}{0.49\textwidth}
\centering
\[
\left(\,
\mathbf{b} \circ \vcenter{\hbox{\begin{tikzpicture}[baseline=-0.5ex]
    \draw (0,0) rectangle (1,2) node[pos=.5] {$U_{\textnormal{trans}}$};
\end{tikzpicture}}}
\,\right)
\times
\left(\,
\mathbf{b} \circ
\vcenter{\hbox{\begin{tikzpicture}[baseline=-0.5ex]
    \draw (0,0) rectangle (1,2) node[pos=.5] {$V_{\textnormal{trans}}$};
\end{tikzpicture}}}
\,\right)^\top
\]
\caption{Unnormalized transition logits}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\[
\left(\,
\mathbf{b} \circ \vcenter{\hbox{\begin{tikzpicture}[baseline=-0.5ex]
    \draw (0,0) rectangle (1,2) node[pos=.5] {$U_{\textnormal{emit}}$};
\end{tikzpicture}}}
\,\right)
\times
\vcenter{\hbox{\begin{tikzpicture}[baseline=-0.5ex]
    \draw (0,0) rectangle (1,2) node[pos=.5] {$V_{\textnormal{emit}}$};
\end{tikzpicture}}}
^\top
\]
\caption{Unnormalized emission logits}
\end{subfigure}
\end{figure}
\end{center}
\end{frame}

\begin{frame}
\frametitle{State Dropout: Inference}
%The cost of inference is reduced by state dropout
\begin{center}
\resizebox{0.8\width}{0.8\height}{
\input{img/trellis_full_drop.tex}
}
\end{center}
\begin{itemize}
\item Shaded nodes depict dropped states
\item Ignore dropped states during inference
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Technique 3: Block-Sparse Emission Constraints}
\begin{itemize}
\item Reduce cost of marginalization by enforcing structure
\vspace{2em}
\item Only allow each word to be emit by a subset of states
\vspace{2em}
\item Cost of inference is quadratic in the size of the largest subset
    due to sparsity
\end{itemize}
(maybe show how this affects $\Lambda_t$? ie multiply masked emission column with $A$)
\end{frame}

\begin{frame}
\frametitle{Block-Sparse Emission Constraints: Alignment}
Start with a joint partitioning of both states and wards
\vspace{2em}

\centering
\begin{columns}
\begin{column}{0.33\textwidth}
\centering
Indices $m \in [M]$
\end{column}
\begin{column}{0.33\textwidth}
\centering
State partitions $\mcZ_m$
\end{column}
\begin{column}{0.33\textwidth}
\centering
Word partitions $\mcX_m$
\end{column}
\end{columns}

\begin{center}
\begin{tikzpicture}[every node/.style={anchor=base,minimum size=8mm}]
    \matrix  (graph) [matrix of nodes, row sep=0.5em,column sep=-0.3em,
    minimum width=2em, minimum height=2em, font=\small,ampersand replacement=\&,
    nodes in empty cells
    ] {
        \&$\mcX_1$ \&
        \&
        \textcolor{green!10}{a}\&
        $\mcX_2$ \&
        \textcolor{green!10}{a}\&
        \&
        $\cdots$ \&
        \&
        $\mcX_M$ \\
        %\textcolor{blue!80}{$p(x\mid z)$} \& \& \& \&\& \&\&\&\&\\
         \& \& \& \&\& \&\&\&\&\\
        \&$\mcZ_1$ \&
        \&
        $\mcZ_2$ \&
        \&
        $\cdots$ \&
        \&
        $\mcZ_M$ \&\&\\
    };
    
    \begin{scope}[on background layer]
      \draw[blue!30, line width=0.5mm] (graph-1-2) -- (graph-3-2);
      \draw[blue!30,line width=0.5mm] (graph-1-5) -- (graph-3-4);
      \draw[blue!30,line width=0.5mm] (graph-1-10) -- (graph-3-8);

      %\draw[draw=red!30, line width=0.2mm] ($(graph-1-1) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      %\draw[draw=red!30, line width=0.2mm] ($(graph-1-2) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      %\draw[draw=red!30, line width=0.1mm] ($(graph-1-3) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      %\draw[draw=red!30, line width=0.9mm] ($(graph-1-4) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      %\draw[draw=red!30, line width=0.1mm] ($(graph-1-5) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      %\draw[draw=red!30, line width=0.4mm] ($(graph-1-6) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);
      %\draw[draw=red!30, line width=0.1mm] ($(graph-1-7) + (0.1, 0)$) -- ($(graph-3-3) + (0.1, 0)$);

      \draw[rounded corners,fill=red!10] ($ (graph-3-2.north west) +(-0.1,0.1)$) rectangle  node[yshift=-0.8cm]{\textcolor{red}{}} ($(graph-3-8.south east) +(0.1,-0.1)$ ) ;
      \draw[rounded corners,fill=red!10] ($ (graph-1-2.north west) +(-0.1,0.1)$) rectangle  node[yshift=0.8cm]{\textcolor{red}{}} ($(graph-1-10.south east) +(0.1,-0.1)$ ) ;

      \draw[rounded corners,fill=green!10] (graph-1-2.north west) rectangle  node[ yshift =0.7cm] {} (graph-1-2.south east);
      \draw[rounded corners,fill=green!10] (graph-1-4.north west) rectangle  node[ yshift =0.7cm] {} (graph-1-6.south east);
      \draw[rounded corners,fill=green!10] (graph-1-10.north west) rectangle  node[ yshift =0.7cm] {} (graph-1-10.south east);

      \draw[rounded corners, fill=blue!10] (graph-3-2.north west) rectangle  node[yshift=-0.9cm]{}  ($(graph-3-2.south east)$);
      \draw[rounded corners, fill=blue!10] (graph-3-4.north west) rectangle  node[yshift=-0.9cm]{}  ($(graph-3-4.south east)$);
      \draw[rounded corners,fill=blue!10] (graph-3-8.north west) rectangle  node[ yshift =0.7cm] {} (graph-3-8.south east);
      %\path[] (graph-3-3.north west) rectangle  node[yshift=-0.9cm]{\textbf{$y_3$}} (graph-3-3.south east);
    \end{scope}
  \end{tikzpicture}
\end{center}


\end{frame}

\begin{frame}
\frametitle{Block-Sparse Emission Constraints}

Given the unnormalized emission logits,
\begin{itemize}
    \item Mask out unaligned state-word entries
    \item Normalize rows across words in aligned partition
\end{itemize}
\begin{center}
\begin{figure}
\scalebox{1}{
\begin{tikzpicture}
%\draw [fill=black] (0,0) rectangle (8,4);

\fill [gray] (0,0) rectangle (7,4);

\draw [fill=white] (0,4) rectangle (1,3);
\draw [fill=white] (1,3) rectangle (4,2);
\draw [fill=white] (4,2) rectangle (6,1);
\draw [fill=white] (6,1) rectangle (7,0);

\draw [thick] (0,0) rectangle (7,4);

%\node at (-.5, 3.5) {$\mcZ_1$};
%\node at (-.5, 2.5) {$\mcZ_2$};
%\node at (-.5, 1.5) {$\mcZ_3$};
%\node at (-.5, .5) {$\mcZ_4$};

%\node at (.5, -.5) {$\mcX_1$};
%\node at (2.5, -.5) {$\mcX_2$};
%\node at (5, -.5) {$\mcX_3$};
%\node at (6.5, -.5) {$\mcX_4$};

\draw[decorate,decoration={brace,mirror},xshift=-0.25em] (0,3.9) -- (0,3.1) node[midway,xshift=-1em] {$\mcZ_1$};
\draw[decorate,decoration={brace,mirror},xshift=-0.25em] (0,2.9) -- (0,2.1) node[midway,xshift=-1em] {$\mcZ_2$};
\draw[decorate,decoration={brace,mirror},xshift=-0.25em] (0,1.9) -- (0,1.1) node[midway,xshift=-1em] {$\mcZ_3$};
\draw[decorate,decoration={brace,mirror},xshift=-0.25em] (0,.9) -- (0,.1) node[midway,xshift=-1em] {$\mcZ_4$};

\draw[decorate,decoration={brace,mirror},yshift=-0.25em] (.1,0) -- (.9,0) node[midway,yshift=-1em] {$\mcX_1$};
\draw[decorate,decoration={brace,mirror},yshift=-0.25em] (1.1,0) -- (3.9,0) node[midway,yshift=-1em] {$\mcX_2$};
\draw[decorate,decoration={brace,mirror},yshift=-0.25em] (4.1,0) -- (5.9,0) node[midway,yshift=-1em] {$\mcX_3$};
\draw[decorate,decoration={brace,mirror},yshift=-0.25em] (6.1,0) -- (6.9,0) node[midway,yshift=-1em] {$\mcX_4$};

\end{tikzpicture}
}
\end{figure}
\end{center}

\end{frame}

\begin{comment}
\begin{frame}
\frametitle{Block-Sparse Emissions: Inference}

Given each word $x_t$, only the states in the correct group can occur
\vspace{1em} 

\begin{figure}
\captionsetup[subfigure]{justification=centering}
\begin{center}
\begin{subfigure}[t]{0.45\textwidth}
\centering
\resizebox{0.75\width}{0.75\height}{
\input{img/diagram_full.tex}
}
\caption{No constraints}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\centering
\resizebox{0.75\width}{0.75\height}{
\input{img/trellis_nodrop.tex}
}
\caption{Block-sparse emission}
\end{subfigure}
\end{center}
\end{figure}

\end{frame}
\end{comment}

\begin{frame}
\frametitle{Block-Sparse Emissions: Inference}

Given each word $x_t$, only the states in the correct group can occur
\vspace{1em} 

\begin{figure}
\captionsetup[subfigure]{justification=centering}
\begin{center}
\begin{subfigure}[t]{0.45\textwidth}
\centering
\resizebox{0.75\width}{0.75\height}{
\input{img/trellis_nodrop.tex}
}
\caption{Block-sparse emission}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\centering
\resizebox{0.75\width}{0.75\height}{
\input{img/trellis.tex}
}
\caption{With state dropout}
\end{subfigure}
\end{center}
\end{figure}

\end{frame}

\begin{frame}
\frametitle{Method Recap}
\begin{itemize}
\item Compact neural parameterization
\vspace{1em}
    \begin{itemize}
    \item[] \FancyUpArrow~Generalization
    \end{itemize}
\vspace{1em}
\item State dropout
\vspace{1em}
    \begin{itemize}
    \item[] \FancyUpArrow~Speed \quad \FancyUpArrow~Generalization
    \end{itemize}
\vspace{1em}
\item Block-sparse emission constraints
\vspace{1em}
    \begin{itemize}
    \item[] \FancyUpArrow~Speed
    \end{itemize}
\vspace{1em}
\item A fourth after experiments
\end{itemize}
\end{frame}


\section{Experiments}
\begin{frame}
\frametitle{Experiments}
\begin{itemize}
\item Language modeling on Penn Treebank
\vspace{1em}
\item Baselines
    \begin{itemize}
    \item Feedforward 5-gram model
    \item 2-layer LSTM
    \item A 900 state HMM (Buys et al 2018)
    \end{itemize}
\vspace{1em}
\item Model
    \begin{itemize}
    \item $2^{15}$ (32k) state very large HMM (VL-HMM)
    \item $M=128$ groups (256 states per type), obtained via Brown Clustering
    \item Dropout rate of $0.5$ during training
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Results on PTB Validation Data}
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar=5pt,
    %x=4cm,
    %enlarge x limits=0.6,
    ylabel={Validation Perplexity},
    symbolic x coords={FF, LSTM, VL-HMM, HMM},
    xticklabels={FF, LSTM, VL-HMM, HMM},
    xtick={FF, LSTM, VL-HMM, HMM},
    tick pos=left,
    every axis plot/.append style={
      ybar,
      bar shift=0pt,
      fill
    },
    nodes near coords,
    nodes near coords align={vertical},
    bar width=2em,
    ]

\addplot coordinates {(FF,159.9)};
\addplot coordinates {(LSTM,93.6)};
\addplot coordinates {(VL-HMM,115.8)};
\addplot coordinates {(HMM,284.6)};

\end{axis}
\end{tikzpicture}
\end{frame}

\begin{frame}
\frametitle{State Size Ablation}

\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel=State Size $Z$,
    ylabel=Validation PPL,
    xmode=log,
    log basis x={2},
    xtick={},
    tick pos=left,
    width=7cm,
    height=5cm
]
\addplot plot coordinates {
(1024,  213.25)
(2048,  199.98)
(4096,  169.18)
(8192,  150.22)
(16384, 135.79)
(32768, 125.02)
(65536, 121.93)
};
\end{axis}
\end{tikzpicture}

\vspace{2em}

Validation perplexity on PTB by state size ($\lambda =0.5$ and $M=128$)
\end{frame}

\begin{frame}
\frametitle{Other Ablations}

\begin{center}
\begin{tabular}{lrrr}
\toprule
Model                 & Param & Train & Val\\
\midrule
VL-HMM ($2^{14}$)     & 7.2M & 115    & 134 \\
\quad - neural param  & 423M & 119    & 169 \\
\quad - state dropout & 7.2M & 88     & 157 \\
%\quad + uniform partition & 7.2M & 83     & 145 \\
\bottomrule
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Greatly scaled the state size of HMMs
\vspace{2em}
\item Performance improved with increasing state size
\vspace{2em}
\item Still a large gap between RNNs and HMMs
\vspace{2em}
\item Does the emission sparsity constraint improve
    computation complexity at the price of accuracy?
\end{itemize}
\end{frame}


\section{Speeding up HMMs with Low-Rank Decompositions}

\begin{frame}
\frametitle{Fast Inference with Low-Rank Decompositions}
\begin{itemize}
\item The previous approach relied a pre-specified emission sparsity constraint
\vspace{2em}
\item Can we scale inference with a weaker constraint?
\vspace{2em}
\item Exploit structure in the transition matrix to speed up inference
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Inference}
Start by unpacking inference to reveal the most expensive step

$$p(x) = \alpha_1^\top\Lambda_2\Lambda_3\cdots\Lambda_T\mathbf{1}$$
with
\begin{align*}
&\textnormal{start, } & [\alpha_1]_{z_1} &= p(x_1\mid z_1)p(z_1),\\
&\textnormal{and transition operators, }
    & [\Lambda_t]_{z_{t-1},z_t} &= p(x_t \mid z_t)p(z_t \mid z_{t-1})
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Inference}
Decompose transition operators into transition matrix $A$ and emission matrix $O$
\begin{align*}
p(x) &= \alpha_1^\top\Lambda_2\cdot\Lambda_T\mathbf{1}\\
&= \alpha_1^\top (A \diag([O]_{\cdot,x_2}))  \cdots\Lambda_T\mathbf{1}\\
&= \alpha_1^\top A \diag([O]_{\cdot,x_2}) \cdots A \diag([O]_{\cdot,x_T}) \mathbf{1}
\end{align*}
where the most expensive steps are the matrix-vector products $\alpha_t^\top A$,
which take $O(Z^2)$ computation

\end{frame}

\begin{frame}
\frametitle{Fast Matrix-Vector Products}
\begin{itemize}
\item Goal is to reduce the naive matvec complexity of $O(Z^2)$
\vspace{2em}
\item Various methods
    \vspace{1em}
    \begin{itemize}
    \item Sparsity (nnz entries)
    \vspace{1em}
    \item Fast Fourier Transform ($Z \log Z$)
    \vspace{1em}
    \item Low-Rank decomposition ($ZR$)
    \end{itemize}
\vspace{2em}
\item We utilize low-rank decompositions
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Low-Rank Factorization}
Factor transition matrix $A$ into product of skinny matrices
\[
\vcenter{\hbox{\begin{tikzpicture}[baseline=-0.5ex]
    \draw (0,0) rectangle (1,.5) node[pos=.5] {$\alpha^\top$};
\end{tikzpicture}}}
\times
\vcenter{\hbox{\begin{tikzpicture}[baseline=-0.5ex]
    \draw (0,0) rectangle (2,2) node[pos=.5] {$A$};
\end{tikzpicture}}}
=
\vcenter{\hbox{\begin{tikzpicture}[baseline=-0.5ex]
    \draw (0,0) rectangle (1,.5) node[pos=.5] {$\alpha^\top$};
\end{tikzpicture}}}
\times
\vcenter{\hbox{\begin{tikzpicture}[baseline=-0.5ex]
    \draw (0,0) rectangle (1,2) node[pos=.5] {$U$};
\end{tikzpicture}}}
\times 
\vcenter{\hbox{\begin{tikzpicture}[baseline=-0.5ex]
    \draw (0,0) rectangle (2,1) node[pos=.5] {$V^\top$};
\end{tikzpicture}}}
\]
resulting in two matrix-vector products of cost $O(ZR)$ each
\vspace{1em}
\begin{itemize}
\item Constraint: Entries of $A$ must be nonnegative
\vspace{1em}
\item Solution: Use a nonnegative matrix factorization (NMF)
$$A = \phi(U)\phi(V)^\top,$$
with $\phi:\R^D\to\R_+$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Method Recap}
\begin{itemize}
\item Target key $O(Z^2)$ matvec step in inference
\vspace{2em}
\item Use NMF to reduce cost to $O(ZR)$
\vspace{2em}
\item How small can $R$ be relative to $Z$ without sacrificing accuracy?
\end{itemize}
\end{frame}


\section{Experiments}

\begin{frame}
\frametitle{Experiments}
\begin{itemize}
\item Language modeling on PTB
\vspace{2em}
\item Feature map $\phi(\bx) = \exp\left(W\bx\right)$,
with learned $W\in\R^{D \times R}$
\vspace{2em}
\item No sparsity constraints
\end{itemize}
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{Asdf}
\begin{center}
\begin{figure}
\begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{img/lhmm-states-features.png}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{img/lhmm-states-features-dropout.png}
\end{subfigure}
\end{figure}
\end{center}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{Scaling on PTB (Validation)}
\begin{center}
\begin{figure}
\includegraphics[width=\textwidth]{img/lhmm-states-features.png}
\end{figure}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Further Scaling on PTB with Dropout (Validation)}
\begin{center}
\begin{figure}
\includegraphics[width=\textwidth]{img/lhmm-states-features-dropout.png}
\end{figure}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Speed Comparison}
\begin{center}
\begin{figure}
\includegraphics[width=\textwidth]{img/lhmm-states-features-speed.png}
\end{figure}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Conclusion (TODO)}
\begin{itemize}
\item Hopeful that HMMs can be competitive language models
\vspace{2em}
\item Introduced 4 techniques for tackling speed and overfitting
\vspace{2em}
\item Future work will extend to other discrete latent variable models
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{EOS}
\end{frame}


\begin{frame}
\frametitle{Citations}
\bibliographystyle{acl_natbib}
\bibliography{anthology,emnlp2020}
\end{frame}

\begin{frame}
\frametitle{Generalized Softmax}
\begin{itemize}
\item Softmax
$$p(z_t \mid z_{t-1}) = \frac{\exp(\bu_{z_{t-1}}^\top \bv_{z_{t}})}
{\sum_z \exp(\bu_{z_{t-1}}^\top \bv_z)}$$
\vspace{1em}
\item Generalized Softmax
$$p(z_t \mid z_{t-1})
= \frac{K(\bu, \bv)}{\sum_z K(\bu,\bv_z)}
= \frac{\phi(\bu)^\top\phi(\bv)}
    {\sum_z \phi(\bu)^\top\phi(\bv_z)},$$
for positive kernel $K:\R^D\times\R^D\to\R_+$
and feature map $\phi:\R^D\to\R^R$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Generalized Softmax: Inference}
\begin{itemize}
\item The key $O(Z^2)$ step in the forward algorithm:
$$
p(z_t \mid x_{<t}) = \sum_{z_{t-1}} p(z_t \mid z_{t-1})p(z_{t-1} \mid x_{<t})
$$
\vspace{1em}
\item In matrix form, 
\begin{equation*}
\bm\gamma_t = \underbrace{\bm\alpha_{t-1}}_{\R^Z}\underbrace{\Lambda_{}}_{\R^{Z \times Z}},
\end{equation*}
where we have the probability of the
\begin{align*}
&\textnormal{current state, } & [\bm\gamma_{t}]_{z_{t}} &= p(z_{t} \mid x_{< t}),\\
&\textnormal{last state, } & [\bm\alpha_{t-1}]_{z_{t-1}} &= p(z_{t-1} \mid x_{< t}),\\
&\textnormal{transition probability, } & [\Lambda]_{z_{t-1},z_t} &= p(z_t\mid z_{t-1})
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Generalized Softmax: Inference}
\begin{itemize}
\item Use generalized softmax in transition distribution
$$[\Lambda]_{z_{t-1},z_t} = p(z_t\mid z_{t-1}) \propto \phi(\bu_{z_{t-1}})^\top\phi(\bv_{z_t})$$
\item Allows us to apply associative property of matrix multiplication
\begin{align*}
\bm\gamma_t
&= \bm\alpha_{t-1}\Lambda\\
&= \bm\alpha_{t-1}(\textrm{diag}(d)\phi(U)\phi(V)^\top)\\
&= \underbrace{(\bm\alpha_{t-1}\circ d)}_{\R^Z}
\underbrace{\phi(U)}_{\R^{Z \times f}}
\underbrace{\phi(V)^\top}_{\R^{f \times Z}},
\end{align*}
with stacked embeddings $\phi(U),\phi(V) = [\phi(\bv_1), \ldots,\phi(\bv_Z)]$
and normalizing constants $d$
\vspace{1em}
\item Takes $O(Zf)$ time from left to right!
\end{itemize}
\end{frame}


\end{document}
